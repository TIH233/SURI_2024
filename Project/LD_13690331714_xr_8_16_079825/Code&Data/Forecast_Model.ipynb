{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9648f20b4b0f064d",
   "metadata": {},
   "source": [
    "# Notebook for Demand forecasting\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T09:34:11.150696Z",
     "start_time": "2024-08-15T09:34:07.973644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "#import seaborn as sns\n",
    "#sns.set_context(\"paper\", font_scale=1.3)\n",
    "#sns.set_style('white')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(20)\n",
    "from sklearn.metrics._scorer import make_scorer\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf"
   ],
   "id": "4606a6e0d01c56cb",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "467ba09484b4bad0",
   "metadata": {},
   "source": [
    "## Helper Methods "
   ]
  },
  {
   "cell_type": "code",
   "id": "70be4f57cf2e5144",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T09:53:51.377779Z",
     "start_time": "2024-08-15T09:53:51.365181Z"
    }
   },
   "source": [
    "import logging\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# define the behavior of output stream: into the console + into a file\n",
    "class Logger:\n",
    "    def __init__(self, filename):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        file_handler = logging.FileHandler(filename)\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "        \n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        \n",
    "        self.logger.addHandler(file_handler)\n",
    "\n",
    "    def log(self, message):\n",
    "        self.logger.info(message)\n",
    "\n",
    "import sys\n",
    "\n",
    "# Calculate pearson coefficient\n",
    "def spcc(y_true, y_pred, **kwargs):\n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    return corr\n",
    "    \n",
    "# create a slicing window dataset (each window is a sequence of data points that can be used for prediction\n",
    "# Prepare dataset for model training \n",
    "# No Direct Modification\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    #dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "# batch (group) windows for more efficient computing\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset\n",
    "    \n",
    "#Takes a dataframe with the holiday field and returns encoded dataframe.\n",
    "def onehotholiday(select):\n",
    "    X_2 = select[['Holiday']]\n",
    "    # Create a OneHotEncoder object\n",
    "    enc = preprocessing.OneHotEncoder(sparse_output=False)\n",
    "    \n",
    "    # Fit and transform\n",
    "    onehotlabels = enc.fit_transform(X_2)\n",
    "    \n",
    "    # Create column names\n",
    "    column_values = [f'Holiday_{i}' for i in range(onehotlabels.shape[1])]\n",
    "\n",
    "    # Create the dataframe \n",
    "    onehotholiday = pd.DataFrame(data=onehotlabels, columns=column_values)\n",
    "\n",
    "    # Join with original dataset\n",
    "    dataset = select.drop(columns=['Holiday'])\n",
    "    dataset = dataset.join(onehotholiday)\n",
    "    \n",
    "    # Reorder columns to put '2to5' at the end\n",
    "    cols = [col for col in dataset.columns if col != '2to5'] + ['2to5']\n",
    "    dataset = dataset[cols]\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "# Create a dataframe contains past 7(look_back) days sales (a numpy array) as a feature\n",
    "def add_lookback(dataset, look_back, df):\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        a = dataset[i:(i+look_back)]['2to5']\n",
    "        a = a.values\n",
    "        for j in range(len(a)):\n",
    "            df[j][i]= a[j]\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "307d0c3bc3215151",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "id": "eab1947d6ea0a76c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T09:53:58.094989Z",
     "start_time": "2024-08-15T09:53:55.215077Z"
    }
   },
   "source": [
    "np.random.seed(42)\n",
    "# fix random seed for reproducibility\n",
    "# load the dataset\n",
    "dataframe = pd.read_csv('./RestaurantDataVets_All_2to5 (1).csv')\n",
    "data = dataframe.drop(columns=['Index','Group','DMY','MissingPrevDays','DailyAvg','DailyBusyness'])\n",
    "\n",
    "lookback=20\n",
    "dataframe_removed_lookback = data.drop([x for x in range(lookback)])\n",
    "\n",
    "for i in range(lookback):\n",
    "    dataframe_removed_lookback[i] = 1.0\n",
    "    \n",
    "df = dataframe_removed_lookback[['Year', 'Day', 'January','February',\n",
    "                         'March','April','May','June','July',\n",
    "                         'August', 'September', 'October', 'November',\n",
    "                         'December','Sunday', 'Monday', 'Tuesday',\n",
    "                         'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Holiday', 'Carnival', \n",
    "                         'LentFasting','Ramadan','ChristmasSeason',\n",
    "                         'WeeklyAvg','MinSales','MaxSales',\n",
    "                         'WeeklyBusyness',\n",
    "                         0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
    "                          14, 15, 16, 17, 18, 19,      \n",
    "                         '2to5']]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#Objects need to be converted to float due to missing values at load time.\n",
    "#df[\"DailyAvg\"] = df.DailyAvg.astype(float)\n",
    "df[\"WeeklyAvg\"] = df.WeeklyAvg.astype(float)\n",
    "df[\"MinSales\"] = df.MinSales.astype(float)\n",
    "df[\"MaxSales\"] = df.MaxSales.astype(float)\n",
    "#df[\"DailyBusyness\"] = df.DailyBusyness.astype(float)\n",
    "df[\"WeeklyBusyness\"] = df.WeeklyBusyness.astype(float)\n",
    "\n",
    "lb_data = add_lookback(data, lookback, df)\n",
    "lb_data = lb_data.reset_index(drop=True)\n",
    "\n",
    "hotdata = onehotholiday(lb_data)\n",
    "hotdata = hotdata.drop(columns=[14,15,16,17,18,19])\n",
    "\n",
    "hot_numcols = len(hotdata.columns)\n",
    "dataset = hotdata.values\n",
    "\n",
    "lbset=lb_data.values\n",
    "lb_numcols =  len(lb_data.columns)\n",
    "\n",
    "print(\"train_df Shape:\" ,lb_data.shape)\n",
    "print(\"After encoding:\", hotdata.shape)\n",
    "\n",
    "X=dataset[:, 0:hot_numcols-1]\n",
    "y=lbset[:, lb_numcols-7:lb_numcols]\n",
    "\n",
    "scaler = preprocessing.RobustScaler()\n",
    "X = scaler.fit_transform(X,y)\n",
    "\n",
    "sys.stdout.flush()\n",
    "\n",
    "scoring = {\n",
    "        'mae': 'neg_mean_absolute_error',\n",
    "        'mse' : 'neg_mean_squared_error',\n",
    "        'custom': make_scorer(spcc, greater_is_better=True)\n",
    "    }\n",
    "\n",
    "j_arr = []\n",
    "j=70\n",
    "feat_reduction = SelectKBest(f_regression, k=j+1) \n",
    "X_new = feat_reduction.fit_transform(X,y[:,0])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=221, random_state=42, shuffle=False)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "            \n",
    "# meansq = keras.metrics.mean_squared_error(y_test.flatten(), y_pred.flatten()).numpy()\n",
    "# meanabs = keras.metrics.mean_absolute_error(y_test.flatten(), y_pred.flatten()).numpy()\n",
    "# print(\"MSE = \"+str(meansq))\n",
    "# print(\"MAE = \"+str(meanabs))\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df Shape: (1091, 51)\n",
      "After encoding: (1091, 72)\n",
      "(870, 71) (870, 7) (221, 71) (221, 7)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Actual Run"
   ],
   "id": "bef75f55ed76fb9"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa88dd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is:  331146.15625\n",
      "The loss is:  290660.8125\n",
      "The loss is:  585638.6875\n",
      "The loss is:  605075.4375\n",
      "The loss is:  920085.625\n",
      "The loss is:  647891.3125\n",
      "The loss is:  725590.6875\n",
      "The loss is:  830078.8125\n",
      "The loss is:  784741.1875\n",
      "The loss is:  713534.3125\n",
      "The loss is:  630002.625\n",
      "The loss is:  800105.5625\n",
      "The loss is:  625068.5\n",
      "The loss is:  783487.9375\n",
      "The loss is:  538357.625\n",
      "The loss is:  629568.375\n",
      "The loss is:  607629.0625\n",
      "The loss is:  667380.5625\n",
      "The loss is:  687239.6875\n",
      "The loss is:  750077.6875\n",
      "The loss is:  790963.8125\n",
      "The loss is:  639960.0625\n",
      "The loss is:  701708.6875\n",
      "The loss is:  659331.125\n",
      "The loss is:  716275.3125\n",
      "The loss is:  631751.8125\n",
      "The loss is:  769635.9375\n",
      "The loss is:  715487.375\n",
      "The loss is:  654364.625\n",
      "The loss is:  1679842.125\n",
      "The loss is:  1782179.375\n",
      "The loss is:  1886618.625\n",
      "The loss is:  1645270.125\n",
      "The loss is:  1800364.0\n",
      "The loss is:  1645276.0\n",
      "The loss is:  1572248.875\n",
      "The loss is:  1409131.0\n",
      "The loss is:  1325379.375\n",
      "The loss is:  1456026.0\n",
      "The loss is:  1407421.125\n",
      "The loss is:  1563094.25\n",
      "The loss is:  1983975.125\n",
      "The loss is:  1761657.625\n",
      "The loss is:  1546104.75\n",
      "The loss is:  1702159.625\n",
      "The loss is:  1920592.125\n",
      "The loss is:  2033583.875\n",
      "The loss is:  2009150.75\n",
      "The loss is:  1432534.125\n",
      "The loss is:  2333853.75\n",
      "The loss is:  2046327.75\n",
      "The loss is:  1991082.25\n",
      "The loss is:  2129693.25\n",
      "The loss is:  1883295.75\n",
      "The test loss is:  1278127.875\n",
      "The test loss is:  1627153.375\n",
      "The test loss is:  1312372.0\n",
      "The test loss is:  1981552.75\n",
      "The test loss is:  1449393.375\n",
      "The test loss is:  2771374.75\n",
      "The test loss is:  2258983.0\n",
      "The test loss is:  1591815.75\n",
      "The test loss is:  1543848.25\n",
      "The test loss is:  1727889.0\n",
      "The test loss is:  1620366.625\n",
      "The test loss is:  1651032.125\n",
      "The test loss is:  1220883.375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Use a transformer model for sequential predicting\n",
    "class PredictModel(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.preprocess=torch.nn.Linear(1,128)\n",
    "        self.model=torch.nn.Transformer(d_model=128, nhead=1, num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=128, dropout=0.1, activation='gelu')\n",
    "        self.output_layer = torch.nn.Linear(128,7)\n",
    "    \n",
    "    def forward(self, x,out_feature=False):\n",
    "        x=torch.reshape(x,(x.shape[0],x.shape[1],1))\n",
    "        x=self.preprocess(x)\n",
    "        x = self.model(x,x)\n",
    "        feature=x[:,0,:]\n",
    "        x = self.output_layer(feature)\n",
    "        if out_feature:\n",
    "            return x,feature\n",
    "        return x\n",
    "\n",
    "# Use an LSTM model\n",
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.lstm=torch.nn.LSTM(1,128,3)\n",
    "        self.output_layer = torch.nn.Linear(128,7)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=torch.reshape(x,(x.shape[0],x.shape[1],1))\n",
    "        x,_=self.lstm(x)\n",
    "        x=x[:,-1,:]\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "# Use a GRU model\n",
    "class GRUModel(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.gru=torch.nn.GRU(1,128,3)\n",
    "        self.output_layer = torch.nn.Linear(128,7)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=torch.reshape(x,(x.shape[0],x.shape[1],1))\n",
    "        x,_=self.gru(x)\n",
    "        x=x[:,-1,:]\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "model=PredictModel()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn=torch.nn.MSELoss()\n",
    "\n",
    "batch_size=16\n",
    "batches=X_train.shape[0]//batch_size\n",
    "for epoch in range(1):\n",
    "    model=model.train()\n",
    "    for batch_id in range(batches):\n",
    "        x=torch.tensor(X_train[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "        y=torch.tensor(y_train[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "        y_pred=model(x)\n",
    "        loss=loss_fn(y_pred,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(\"The loss is: \",loss.item())\n",
    "\n",
    "    # Test the performance\n",
    "    model=model.eval()\n",
    "    test_batches=X_test.shape[0]//batch_size\n",
    "    for batch_id in range(test_batches):\n",
    "        x=torch.tensor(X_test[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "        y=torch.tensor(y_test[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "        x=torch.tensor(x).float()\n",
    "        y=torch.tensor(y).float()\n",
    "        y_pred=model(x)\n",
    "        loss=loss_fn(y_pred,y)\n",
    "        print(\"The test loss is: \",loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29d2a09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(864, 128) (208, 128)\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to get features\n",
    "features_X_train=[]\n",
    "features_X_test=[]\n",
    "\n",
    "model=model.eval()\n",
    "batch_size=16\n",
    "batches=X_train.shape[0]//batch_size\n",
    "for batch_id in range(batches):\n",
    "    x=torch.tensor(X_train[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "    y=torch.tensor(y_train[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "    y_pred,feature=model(x,out_feature=True)\n",
    "    feature=feature.detach().numpy()\n",
    "    for i in range(feature.shape[0]):\n",
    "        features_X_train.append(feature[i])\n",
    "features_X_train=np.array(features_X_train)\n",
    "\n",
    "test_batches=X_test.shape[0]//batch_size\n",
    "for batch_id in range(test_batches):\n",
    "    x=torch.tensor(X_test[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "    y=torch.tensor(y_test[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "    y_pred,feature=model(x,out_feature=True)\n",
    "    feature=feature.detach().numpy()\n",
    "    for i in range(feature.shape[0]):\n",
    "        features_X_test.append(feature[i])\n",
    "features_X_test=np.array(features_X_test)\n",
    "\n",
    "print(features_X_train.shape,features_X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdd89aef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 71 features, but KNeighborsRegressor is expecting 128 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m y_train\u001B[38;5;241m=\u001B[39my_train[:features_X_train\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m],\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]\n\u001B[1;32m      6\u001B[0m neigh\u001B[38;5;241m.\u001B[39mfit(features_X_train, y_train)\n\u001B[0;32m----> 7\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mneigh\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m meansq \u001B[38;5;241m=\u001B[39m keras\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mmean_squared_error(y_test\u001B[38;5;241m.\u001B[39mflatten(), y_pred\u001B[38;5;241m.\u001B[39mflatten())\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m     10\u001B[0m meanabs \u001B[38;5;241m=\u001B[39m keras\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mmean_absolute_error(y_test\u001B[38;5;241m.\u001B[39mflatten(), y_pred\u001B[38;5;241m.\u001B[39mflatten())\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/sklearn/multioutput.py:305\u001B[0m, in \u001B[0;36m_MultiOutputEstimator.predict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredict\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    303\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe base estimator should implement a predict method\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 305\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    306\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimators_\u001B[49m\n\u001B[1;32m    307\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39masarray(y)\u001B[38;5;241m.\u001B[39mT\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     60\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     61\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     62\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     64\u001B[0m )\n\u001B[0;32m---> 65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/joblib/parallel.py:1863\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1861\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[1;32m   1862\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1865\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[1;32m   1866\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[1;32m   1867\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[1;32m   1868\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[1;32m   1869\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[1;32m   1870\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/joblib/parallel.py:1792\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1790\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1791\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1792\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1793\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1794\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/sklearn/utils/parallel.py:127\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    125\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[0;32m--> 127\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/sklearn/neighbors/_regression.py:237\u001B[0m, in \u001B[0;36mKNeighborsRegressor.predict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Predict the target for the provided data.\u001B[39;00m\n\u001B[1;32m    222\u001B[0m \n\u001B[1;32m    223\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;124;03m    Target values.\u001B[39;00m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muniform\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;66;03m# In that case, we do not need the distances to perform\u001B[39;00m\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;66;03m# the weighting so we do not compute them.\u001B[39;00m\n\u001B[0;32m--> 237\u001B[0m     neigh_ind \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkneighbors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_distance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    238\u001B[0m     neigh_dist \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/sklearn/neighbors/_base.py:804\u001B[0m, in \u001B[0;36mKNeighborsMixin.kneighbors\u001B[0;34m(self, X, n_neighbors, return_distance)\u001B[0m\n\u001B[1;32m    802\u001B[0m         X \u001B[38;5;241m=\u001B[39m _check_precomputed(X)\n\u001B[1;32m    803\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 804\u001B[0m         X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    806\u001B[0m n_samples_fit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_samples_fit_\n\u001B[1;32m    807\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_neighbors \u001B[38;5;241m>\u001B[39m n_samples_fit:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/sklearn/base.py:626\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[1;32m    623\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m--> 626\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_n_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/sklearn/base.py:415\u001B[0m, in \u001B[0;36mBaseEstimator._check_n_features\u001B[0;34m(self, X, reset)\u001B[0m\n\u001B[1;32m    412\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    414\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_features \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_features_in_:\n\u001B[0;32m--> 415\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    416\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX has \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_features\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m features, but \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    417\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis expecting \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_features_in_\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m features as input.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    418\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: X has 71 features, but KNeighborsRegressor is expecting 128 features as input."
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from tensorflow import keras\n",
    "neigh = MultiOutputRegressor(KNeighborsRegressor(n_neighbors=7))\n",
    "y_train=y_train[:features_X_train.shape[0],...]\n",
    "neigh.fit(features_X_train, y_train)\n",
    "y_test=y_test[:features_X_test.shape[0],...]\n",
    "y_pred = neigh.predict(features_X_test)\n",
    "            \n",
    "meansq = keras.metrics.mean_squared_error(y_test.flatten(), y_pred.flatten()).numpy()\n",
    "meanabs = keras.metrics.mean_absolute_error(y_test.flatten(), y_pred.flatten()).numpy()\n",
    "print(\"MSE = \"+str(meansq))\n",
    "print(\"MAE = \"+str(meanabs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
