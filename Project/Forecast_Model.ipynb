{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9648f20b4b0f064d",
   "metadata": {},
   "source": [
    "# Notebook for Demand forecasting\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4606a6e0d01c56cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T03:30:04.790953Z",
     "start_time": "2024-08-11T03:30:04.769371Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 18:06:40.407157: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-03 18:06:40.428717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "#import seaborn as sns\n",
    "#sns.set_context(\"paper\", font_scale=1.3)\n",
    "#sns.set_style('white')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(20)\n",
    "from sklearn.metrics._scorer import make_scorer\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ba09484b4bad0",
   "metadata": {},
   "source": [
    "## Helper Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70be4f57cf2e5144",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T03:30:14.673298Z",
     "start_time": "2024-08-11T03:30:14.663791Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, filename):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        file_handler = logging.FileHandler(filename)\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "        \n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        \n",
    "        self.logger.addHandler(file_handler)\n",
    "\n",
    "    def log(self, message):\n",
    "        self.logger.info(message)\n",
    "\n",
    "import sys\n",
    "\n",
    "def spcc(y_true, y_pred, **kwargs):\n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    return corr\n",
    "    \n",
    "    \n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    #dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset\n",
    "    \n",
    "#Takes a dataframe with the holiday field and returns encoded dataframe.\n",
    "def onehotholiday(select):\n",
    "    X_2 = select[['Holiday']]\n",
    "    # TODO: create a OneHotEncoder object, and fit it to all of X\n",
    "    # 1. INSTANTIATE\n",
    "    enc = preprocessing.OneHotEncoder(sparse=False)\n",
    "    \n",
    "    # 2. FIT\n",
    "    enc.fit(X_2)\n",
    "    \n",
    "    # 3. Transform\n",
    "    onehotlabels = enc.transform(X_2)\n",
    "    # creating a list of column names \n",
    "    column_values = []\n",
    "    for i in range(np.shape(onehotlabels)[1]):\n",
    "            column_values.append('A'+str(i))\n",
    "\n",
    "    # creating the dataframe \n",
    "    onehotholiday = pd.DataFrame(data = onehotlabels,columns = column_values)\n",
    "\n",
    "    dataset = select.drop(columns=['Holiday'])\n",
    "    dataset = select.join(onehotholiday)\n",
    "    df1 = dataset.pop('2to5')\n",
    "    dataset['2to5']=df1 # add b series as a 'new' column\n",
    "    dataset2=dataset\n",
    "    dataset2 = dataset2.drop(columns=['Holiday'])\n",
    "    return dataset2\n",
    "    \n",
    "\n",
    "def add_lookback(dataset, look_back, df):\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        a = dataset[i:(i+look_back)]['2to5']\n",
    "        a = a.values\n",
    "        for j in range(len(a)):\n",
    "            df[j][i]= a[j]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307d0c3bc3215151",
   "metadata": {},
   "source": [
    "## Actual Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab1947d6ea0a76c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/hty/Documents/SURI/SURI/RestaurantDataVets_All_2to5.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# fix random seed for reproducibility\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# load the dataset\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~/Documents/SURI/SURI/RestaurantDataVets_All_2to5.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m dataframe\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndex\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGroup\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDMY\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissingPrevDays\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDailyAvg\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDailyBusyness\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m lookback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/SURI_env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/SURI_env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/SURI_env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/envs/SURI_env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/SURI_env/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/hty/Documents/SURI/SURI/RestaurantDataVets_All_2to5.csv'"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# fix random seed for reproducibility\n",
    "# load the dataset\n",
    "dataframe = pd.read_csv('~/Documents/SURI/SURI/RestaurantDataVets_All_2to5.csv')\n",
    "data = dataframe.drop(columns=['Index','Group','DMY','MissingPrevDays','DailyAvg','DailyBusyness'])\n",
    "\n",
    "lookback=20\n",
    "dataframe_removed_lookback = data.drop([x for x in range(lookback)])\n",
    "\n",
    "for i in range(lookback):\n",
    "    dataframe_removed_lookback[i] = 1.0\n",
    "# first 20 rows dropped and 20 new columns added filled with 1 (dummy variables)\n",
    "\n",
    "df = dataframe_removed_lookback[['Year', 'Day', 'January','February',\n",
    "                         'March','April','May','June','July',\n",
    "                         'August', 'September', 'October', 'November',\n",
    "                         'December','Sunday', 'Monday', 'Tuesday',\n",
    "                         'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Holiday', 'Carnival', \n",
    "                         'LentFasting','Ramadan','ChristmasSeason',\n",
    "                         'WeeklyAvg','MinSales','MaxSales',\n",
    "                         'WeeklyBusyness',\n",
    "                         0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
    "                          14, 15, 16, 17, 18, 19,      \n",
    "                         '2to5']]\n",
    "df = df.reset_index(drop=True)\n",
    "# select features from dataframe and reset index\n",
    "\n",
    "#Objects need to be converted to float due to missing values at load time.\n",
    "#df[\"DailyAvg\"] = df.DailyAvg.astype(float)\n",
    "df[\"WeeklyAvg\"] = df.WeeklyAvg.astype(float)\n",
    "df[\"MinSales\"] = df.MinSales.astype(float)\n",
    "df[\"MaxSales\"] = df.MaxSales.astype(float)\n",
    "#df[\"DailyBusyness\"] = df.DailyBusyness.astype(float)\n",
    "df[\"WeeklyBusyness\"] = df.WeeklyBusyness.astype(float)\n",
    "\n",
    "lb_data = add_lookback(data, lookback, df)  \n",
    "lb_data = lb_data.reset_index(drop=True)\n",
    "\n",
    "hotdata = onehotholiday(lb_data)\n",
    "hotdata = hotdata.drop(columns=[14,15,16,17,18,19])\n",
    "\n",
    "hot_numcols = len(hotdata.columns)\n",
    "dataset = hotdata.values\n",
    "\n",
    "lbset=lb_data.values\n",
    "lb_numcols =  len(lb_data.columns)\n",
    "\n",
    "print(\"train_df Shape:\" ,lb_data.shape)\n",
    "print(\"After encoding:\", hotdata.shape)\n",
    "\n",
    "X=dataset[:, 0:hot_numcols-1]\n",
    "y=lbset[:, lb_numcols-7:lb_numcols]\n",
    "\n",
    "scaler = preprocessing.RobustScaler()\n",
    "X = scaler.fit_transform(X,y)\n",
    "\n",
    "sys.stdout.flush()\n",
    "\n",
    "scoring = {\n",
    "        'mae': 'neg_mean_absolute_error',\n",
    "        'mse' : 'neg_mean_squared_error',\n",
    "        'custom': make_scorer(spcc, greater_is_better=True)\n",
    "    }\n",
    "\n",
    "j_arr = []\n",
    "j=70\n",
    "feat_reduction = SelectKBest(f_regression, k=j+1) \n",
    "X_new = feat_reduction.fit_transform(X,y[:,0])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=221, random_state=42, shuffle=False)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "            \n",
    "# meansq = keras.metrics.mean_squared_error(y_test.flatten(), y_pred.flatten()).numpy()\n",
    "# meanabs = keras.metrics.mean_absolute_error(y_test.flatten(), y_pred.flatten()).numpy()\n",
    "# print(\"MSE = \"+str(meansq))\n",
    "# print(\"MAE = \"+str(meanabs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa88dd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is:  331146.15625\n",
      "The loss is:  290660.8125\n",
      "The loss is:  585638.6875\n",
      "The loss is:  605075.4375\n",
      "The loss is:  920085.625\n",
      "The loss is:  647891.3125\n",
      "The loss is:  725590.6875\n",
      "The loss is:  830078.8125\n",
      "The loss is:  784741.1875\n",
      "The loss is:  713534.3125\n",
      "The loss is:  630002.625\n",
      "The loss is:  800105.5625\n",
      "The loss is:  625068.5\n",
      "The loss is:  783487.9375\n",
      "The loss is:  538357.625\n",
      "The loss is:  629568.375\n",
      "The loss is:  607629.0625\n",
      "The loss is:  667380.5625\n",
      "The loss is:  687239.6875\n",
      "The loss is:  750077.6875\n",
      "The loss is:  790963.8125\n",
      "The loss is:  639960.0625\n",
      "The loss is:  701708.6875\n",
      "The loss is:  659331.125\n",
      "The loss is:  716275.3125\n",
      "The loss is:  631751.8125\n",
      "The loss is:  769635.9375\n",
      "The loss is:  715487.375\n",
      "The loss is:  654364.625\n",
      "The loss is:  1679842.125\n",
      "The loss is:  1782179.375\n",
      "The loss is:  1886618.625\n",
      "The loss is:  1645270.125\n",
      "The loss is:  1800364.0\n",
      "The loss is:  1645276.0\n",
      "The loss is:  1572248.875\n",
      "The loss is:  1409131.0\n",
      "The loss is:  1325379.375\n",
      "The loss is:  1456026.0\n",
      "The loss is:  1407421.125\n",
      "The loss is:  1563094.25\n",
      "The loss is:  1983975.125\n",
      "The loss is:  1761657.625\n",
      "The loss is:  1546104.75\n",
      "The loss is:  1702159.625\n",
      "The loss is:  1920592.125\n",
      "The loss is:  2033583.875\n",
      "The loss is:  2009150.75\n",
      "The loss is:  1432534.125\n",
      "The loss is:  2333853.75\n",
      "The loss is:  2046327.75\n",
      "The loss is:  1991082.25\n",
      "The loss is:  2129693.25\n",
      "The loss is:  1883295.75\n",
      "The test loss is:  1278127.875\n",
      "The test loss is:  1627153.375\n",
      "The test loss is:  1312372.0\n",
      "The test loss is:  1981552.75\n",
      "The test loss is:  1449393.375\n",
      "The test loss is:  2771374.75\n",
      "The test loss is:  2258983.0\n",
      "The test loss is:  1591815.75\n",
      "The test loss is:  1543848.25\n",
      "The test loss is:  1727889.0\n",
      "The test loss is:  1620366.625\n",
      "The test loss is:  1651032.125\n",
      "The test loss is:  1220883.375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Use a transformer model for sequential predicting\n",
    "class PredictModel(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.preprocess=torch.nn.Linear(1,128)\n",
    "        self.model=torch.nn.Transformer(d_model=128, nhead=1, num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=128, dropout=0.1, activation='gelu')\n",
    "        self.output_layer = torch.nn.Linear(128,7)\n",
    "    \n",
    "    def forward(self, x,out_feature=False):\n",
    "        x=torch.reshape(x,(x.shape[0],x.shape[1],1))\n",
    "        x=self.preprocess(x)\n",
    "        x = self.model(x,x)\n",
    "        feature=x[:,0,:]\n",
    "        x = self.output_layer(feature)\n",
    "        if out_feature:\n",
    "            return x,feature\n",
    "        return x\n",
    "\n",
    "# Use an LSTM model\n",
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.lstm=torch.nn.LSTM(1,128,3)\n",
    "        self.output_layer = torch.nn.Linear(128,7)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=torch.reshape(x,(x.shape[0],x.shape[1],1))\n",
    "        x,_=self.lstm(x)\n",
    "        x=x[:,-1,:]\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "# Use a GRU model\n",
    "class GRUModel(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.gru=torch.nn.GRU(1,128,3)\n",
    "        self.output_layer = torch.nn.Linear(128,7)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=torch.reshape(x,(x.shape[0],x.shape[1],1))\n",
    "        x,_=self.gru(x)\n",
    "        x=x[:,-1,:]\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "model=PredictModel()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn=torch.nn.MSELoss()\n",
    "\n",
    "batch_size=16\n",
    "batches=X_train.shape[0]//batch_size\n",
    "for epoch in range(1):\n",
    "    model=model.train()\n",
    "    for batch_id in range(batches):\n",
    "        x=torch.tensor(X_train[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "        y=torch.tensor(y_train[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "        y_pred=model(x)\n",
    "        loss=loss_fn(y_pred,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(\"The loss is: \",loss.item())\n",
    "\n",
    "    # Test the performance\n",
    "    model=model.eval()\n",
    "    test_batches=X_test.shape[0]//batch_size\n",
    "    for batch_id in range(test_batches):\n",
    "        x=torch.tensor(X_test[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "        y=torch.tensor(y_test[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "        x=torch.tensor(x).float()\n",
    "        y=torch.tensor(y).float()\n",
    "        y_pred=model(x)\n",
    "        loss=loss_fn(y_pred,y)\n",
    "        print(\"The test loss is: \",loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29d2a09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(864, 128) (208, 128)\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to get features\n",
    "features_X_train=[]\n",
    "features_X_test=[]\n",
    "\n",
    "model=model.eval()\n",
    "batch_size=16\n",
    "batches=X_train.shape[0]//batch_size\n",
    "for batch_id in range(batches):\n",
    "    x=torch.tensor(X_train[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "    y=torch.tensor(y_train[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "    y_pred,feature=model(x,out_feature=True)\n",
    "    feature=feature.detach().numpy()\n",
    "    for i in range(feature.shape[0]):\n",
    "        features_X_train.append(feature[i])\n",
    "features_X_train=np.array(features_X_train)\n",
    "\n",
    "test_batches=X_test.shape[0]//batch_size\n",
    "for batch_id in range(test_batches):\n",
    "    x=torch.tensor(X_test[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "    y=torch.tensor(y_test[batch_id*batch_size:(batch_id+1)*batch_size]).float()\n",
    "    y_pred,feature=model(x,out_feature=True)\n",
    "    feature=feature.detach().numpy()\n",
    "    for i in range(feature.shape[0]):\n",
    "        features_X_test.append(feature[i])\n",
    "features_X_test=np.array(features_X_test)\n",
    "\n",
    "print(features_X_train.shape,features_X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdd89aef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 71 features, but KNeighborsRegressor is expecting 128 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m y_train\u001b[38;5;241m=\u001b[39my_train[:features_X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m      6\u001b[0m neigh\u001b[38;5;241m.\u001b[39mfit(features_X_train, y_train)\n\u001b[0;32m----> 7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mneigh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m meansq \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mmean_squared_error(y_test\u001b[38;5;241m.\u001b[39mflatten(), y_pred\u001b[38;5;241m.\u001b[39mflatten())\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     10\u001b[0m meanabs \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mmean_absolute_error(y_test\u001b[38;5;241m.\u001b[39mflatten(), y_pred\u001b[38;5;241m.\u001b[39mflatten())\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/sklearn/multioutput.py:305\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe base estimator should implement a predict method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 305\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(y)\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/sklearn/neighbors/_regression.py:237\u001b[0m, in \u001b[0;36mKNeighborsRegressor.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict the target for the provided data.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    Target values.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/sklearn/neighbors/_base.py:804\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    802\u001b[0m         X \u001b[38;5;241m=\u001b[39m _check_precomputed(X)\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 804\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/sklearn/base.py:626\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/plato/lib/python3.10/site-packages/sklearn/base.py:415\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 71 features, but KNeighborsRegressor is expecting 128 features as input."
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from tensorflow import keras\n",
    "neigh = MultiOutputRegressor(KNeighborsRegressor(n_neighbors=7))\n",
    "y_train=y_train[:features_X_train.shape[0],...]\n",
    "neigh.fit(features_X_train, y_train)\n",
    "y_test=y_test[:features_X_test.shape[0],...]\n",
    "y_pred = neigh.predict(features_X_test)\n",
    "            \n",
    "meansq = keras.metrics.mean_squared_error(y_test.flatten(), y_pred.flatten()).numpy()\n",
    "meanabs = keras.metrics.mean_absolute_error(y_test.flatten(), y_pred.flatten()).numpy()\n",
    "print(\"MSE = \"+str(meansq))\n",
    "print(\"MAE = \"+str(meanabs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
